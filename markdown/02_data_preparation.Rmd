---
title: "Data preparation"
date: "September, 2019"
---

```{r setup_preparation, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.pos =  "h")
knitr::opts_knit$set(root.dir = "../")

# libraries for spatial data manipulation
library(rgdal)
library(raster)
library(spdep)
library(bamlss)
library(shapefiles)
```

```{r scripts_data_preparation, include=FALSE}
# loading required steps before performing the analysis
source("./src/datapreparation/step_01_create_functions.R")
source("./src/datapreparation/step_02_data_ingestion.R")
source("./src/datapreparation/step_03_data_cleaning.R")
```

# Using a step by step approach

Before starting the analysis for the {{Spatial-Statistics}}, a few important steps were taken in order to prepare the source data files. These steps are listed below:

- **Step 01**: Create Functions;
- **Step 02**: Data Ingestion;
- **Step 03**: Data Cleaning;
- **Step 04**: Label Translation;
- **Step 05**: Data Enhancement;
- **Step 06**: Dataset Preparation.

*******************************************************************************

# Create Functions (step 1)
This step create functions to be used in the next steps. Following, all functions created are described.

## Function 1
Description of function 1.

``` {r function_1_name, eval = FALSE}
# Function 1 code
```

## Function 2
Description of function 2.

``` {r function_2_name, eval = FALSE}
# Function 2 code
```

## Function 3
Description of function 3.

``` {r function_3_name, eval = FALSE}
# Function 3 code
```

## Function n
Description of function n.

``` {r function_n_name, eval = FALSE}
# Function n code
```

*******************************************************************************

# Data Ingestion (step 2)
The process of data ingestion — preparing data for analysis — usually includes steps called extract (taking the data from its current location), transform (cleansing and normalizing the data), and load (placing the data in a database where it can be analyzed).

During this step, in addition to the loading data processes, it was performed data casting, column renaming and small touch-ups. The list below describe each table adjustment taken:

- **Table 1**: short description about what was done in the table 1;
- **Table 2**: short description about what was done in the table 2;
- **Table 3**: short description about what was done in the table 3;
- **Table n**: short description about what was done in the table n;

*******************************************************************************

# Data Cleaning (step 3)

The objective of this step is analysing missing values and other strange conditions. In order to accomplish this task, a few R functions were used to quickly discover missing values, like NA and empty fields.

First thing done, was fixing observations in k_symbol transaction table with ' ' (one space) to empty string (''), using the following command.

Then, the command below was used to find out any NA values in each table.

``` {r find_na, eval = FALSE}
sapply(TableName, function(x) sum(is.na(x)))
```

Solely the **transaction** table has NA values, in the following columns:

```{r transaction_na_cols, echo=FALSE, results = 'asis'}
kable(mytable_na_cols)
```

Finally, the following command was used in each table to find out where empty values was hidden. 

``` {r find_empty, eval = FALSE}
sapply(TableName, function(x) table(as.character(x) =="")["TRUE"])
```

Again, only the **transaction** table had empty values, according to the table below:

```{r echo=FALSE, results = 'asis'}
kable(mytable_empty_cols)
```

For the exploration analysis report, we did not take any additional action, since the missing values was not relevant.

*******************************************************************************

# Label Translation (step 4)
In order to make the data information more understandable, it was translated some relevant labels and domains from Czech to English.

``` {r translate, eval = FALSE}
# Translating relevant labels and domains to english --------------------------------------------

mytable$x_var_1 <- plyr::mapvalues(mytable$x_var_1, 
                                   c('Other_Language_Column_Name1', 
                                     'Other_Language_Column_Name2', 
                                     'Other_Language_Column_Name3'),
                                   c('English_Column_Name1', 
                                     'English_Column_Name2', 
                                     'English_Column_Name3'))

```

*******************************************************************************

# Data Enhancement (step 5)
This step aims to improve the analysis by adding auxiliary information. Data enhancement is all about making sure any data that is coming into the business is being looked at with a critical eye and is being filtered down to maximize its value.

<Describe what kind of data enhancement you have to do. Include chuncks of code to ilustrate what is necessary.>

``` {r client, eval = FALSE}
# your code with the enhancements goes here
```

*******************************************************************************

# Data Preparation for Predictive Modeling (step 6)

## Selecting the target dataset

The below function was created to be used in the modeling exercises to be performed, the idea is to have a standard way to get the prepared data set already prepared with correct data types and dummies.

``` {r data_prep, eval = FALSE}
# your code with the data preparation goes here
```

## Splitting dataset into Train and Test data
The below function was created to be used in the modeling exercises to be split the source_dataset into train and test datasets.

``` {r split_func, eval = FALSE}
mydataset <- fgvr::createTestAndTrainSamples(dataset = you_dataset_name, yvar = "your_y_var", 
                                             seed = 12345, percentage = 0.7)
```

To make sure all the models uses the same datasets for Train and Testing we are saving the initial sampling to be reused across the models.

This will ensure consistency when comparing the models against each other.

``` {r save_dataset, eval = FALSE}
# calling function to split and create train and test databases
# this function will split the dataset into train and test data and save the sampling in disk
# to resample just delete './models/source_train_test_dataset.rds' file and rerun this script
if (file.exists('./models/source_train_test_dataset.rds')) {
  source_train_test_dataset <- readRDS('./models/source_train_test_dataset.rds')
} else {
  source_train_test_dataset <- mydataset
  saveRDS(source_train_test_dataset, './models/source_train_test_dataset.rds')  
}
```
